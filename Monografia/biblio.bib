@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{alan1950a,
  title = {Computing Machinery and Intelligence},
  year = {1950},
  journal = {Mind},
  pages = {433-460},
  volume = {49},
  author = {Turing, Alan Mathison}
}

@article{searle1980minds,
  title={Minds, brains, and programs},
  author={Searle, John R.},
  journal={Behavioral and Brain Sciences},
  volume={3},
  number={03},
  pages={417--424},
  year={1980},
  publisher={Cambridge University Press}
}

@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM New York, NY, USA}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{sparck1972statistical,
  title={A statistical interpretation of term specificity and its application in retrieval},
  author={Sparck Jones, Karen},
  journal={Journal of documentation},
  volume={28},
  number={1},
  pages={11--21},
  year={1972},
  publisher={MCB UP Ltd}
}

@misc{openai2022chatgpt,
  author       = {OpenAI},
  title        = {ChatGPT: Optimizing Language Models for Dialogue},
  year         = {2022},
  note         = {\url{https://openai.com/blog/chatgpt}},
}

@misc{google2023gemini,
  author       = {Google DeepMind},
  title        = {Gemini: Our Largest and Most Capable AI Models},
  year         = {2023},
  note         = {\url{https://deepmind.google/technologies/gemini/}},
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@article{bert-score,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@inproceedings{lin2004rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{papineni2002bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040/",
    doi = "10.3115/1073083.1073135",
    pages = "311--318"
}

@inproceedings{banerjee2005meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909/",
    pages = "65--72"
}

@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27263--27277},
  year={2021}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{kusner2015word,
  title = 	 {From Word Embeddings To Document Distances},
  author = 	 {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {957--966},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/kusnerb15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/kusnerb15.html},
  abstract = 	 {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.}
}


@inproceedings{koehn2006manual,
    title = "Manual and Automatic Evaluation of Machine Translation between {E}uropean Languages",
    author = "Koehn, Philipp  and
      Monz, Christof",
    editor = "Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings on the Workshop on Statistical Machine Translation",
    month = jun,
    year = "2006",
    address = "New York City",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-3114/",
    pages = "102--121"
}

@misc{nist2006nist,
  title={NIST 2006 machine translation evaluation official results},
  author={NIST, November},
  year={2006}
}

@inproceedings{galley2015deltableu,
    title = "delta{BLEU}: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets",
    author = "Galley, Michel  and
      Brockett, Chris  and
      Sordoni, Alessandro  and
      Ji, Yangfeng  and
      Auli, Michael  and
      Quirk, Chris  and
      Mitchell, Margaret  and
      Gao, Jianfeng  and
      Dolan, Bill",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-2073/",
    doi = "10.3115/v1/P15-2073",
    pages = "445--450"
}

@inproceedings{liu2016how,
    title = "How {NOT} To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
    author = "Liu, Chia-Wei  and
      Lowe, Ryan  and
      Serban, Iulian  and
      Noseworthy, Mike  and
      Charlin, Laurent  and
      Pineau, Joelle",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1230/",
    doi = "10.18653/v1/D16-1230",
    pages = "2122--2132"
}

@article{fabbri2021summeval,
    title = "{S}umm{E}val: Re-evaluating Summarization Evaluation",
    author = "Fabbri, Alexander R.  and
      Kry{\'s}ci{\'n}ski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard  and
      Radev, Dragomir",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.24/",
    doi = "10.1162/tacl_a_00373",
    pages = "391--409",
    abstract = "The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments."
}

@InProceedings{ptt5summ_bracis,
  author="Paiola, Pedro H.
    and de Rosa, Gustavo H.
    and Papa, Jo{\~a}o P.",
  editor="Xavier-Junior, Jo{\~a}o Carlos
    and Rios, Ricardo Ara{\'u}jo",
  title="Deep Learning-Based Abstractive Summarization for Brazilian Portuguese Texts",
  booktitle="BRACIS 2022: Intelligent Systems",
  year="2022",
  publisher="Springer International Publishing",
  address="Cham",
  pages="479--493",
  isbn="978-3-031-21689-3"
}

@misc{recogna_institution,
  author       = {{Recogna}},
  title        = {Recogna: Biometric \& Pattern Recognition Research Group},
  howpublished = {Website institucional},
  year         = {2025},
  note         = {Disponível em: recogna.tech},
}

@article{hasan2021xl,
  title={XL-sum: Large-scale multilingual abstractive summarization for 44 languages},
  author={Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Samin, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M Sohel and Shahriyar, Rifat},
  journal={arXiv preprint arXiv:2106.13822},
  year={2021}
}

@misc{recogna_ptt5summ,
  author = {Recogna},
  title = {ptt5-base-summarizer-xl-sum-ptbr},
  year = {2023},
  howpublished = {\url{https://huggingface.co/recogna/ptt5-base-summarizer-xl-sum-ptbr}},
  note = {Disponível na plataforma Hugging Face}
}

@misc{arubenruben2023cnnpt,
  author       = {Ruben, Andre},
  title        = {cnn\_dailymail\_azure\_pt\_pt},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/arubenruben/cnn_dailymail_azure_pt_pt}},
  note         = {Dataset traduzido do CNN/DailyMail para português, disponível na plataforma Hugging Face}
}

@inproceedings{nallapati2016abstractive,
    title = "Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond",
    author = "Nallapati, Ramesh  and
      Zhou, Bowen  and
      dos Santos, Cicero  and
      Gu{\ensuremath{\dot{}}}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar  and
      Xiang, Bing",
    editor = "Riezler, Stefan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1028/",
    doi = "10.18653/v1/K16-1028",
    pages = "280--290"
}

@misc{leite2024fairytaleqa_ptbr,
  author       = {Leite, Benjamim},
  title        = {FairytaleQA-translated-ptBR},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/benjleite/FairytaleQA-translated-ptBR}},
  note         = {Versão traduzida para o português brasileiro do dataset FairytaleQA, disponível na plataforma Hugging Face}
}

@misc{xu2022fairytaleqa,
      title={Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension}, 
      author={Ying Xu and Dakuo Wang and Mo Yu and Daniel Ritchie and Bingsheng Yao and Tongshuang Wu and Zheng Zhang and Toby Jia-Jun Li and Nora Bradford and Branda Sun and Tran Bao Hoang and Yisi Sang and Yufang Hou and Xiaojuan Ma and Diyi Yang and Nanyun Peng and Zhou Yu and Mark Warschauer},
      year={2022},
      eprint={2203.13947},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.13947}, 
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{zhao2019moverscore,
  title={MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance},
  author={Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M and Eger, Steffen},
  journal={arXiv preprint arXiv:1909.02622},
  year={2019}
}

@article{schober2018correlation,
  title={Correlation coefficients: appropriate use and interpretation},
  author={Schober, Patrick and Boer, Christa and Schwarte, Lothar A},
  journal={Anesthesia \& analgesia},
  volume={126},
  number={5},
  pages={1763--1768},
  year={2018},
  publisher={LWW}
}

@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{souza2020bertimbau,
    author = {Souza, F\'{a}bio and Nogueira, Rodrigo and Lotufo, Roberto},
    title = {BERTimbau: Pretrained BERT Models for Brazilian Portuguese},
    year = {2020},
    isbn = {978-3-030-61376-1},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-030-61377-8_28},
    doi = {10.1007/978-3-030-61377-8_28},
    abstract = {Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of large pretrained language models (LMs) to downstream natural language processing (NLP) tasks. This transfer learning approach improves the overall performance on many tasks and is highly beneficial when labeled data is scarce, making pretrained LMs valuable resources specially for languages with few annotated training examples. In this work, we train BERT (Bidirectional Encoder Representations from Transformers) models for Brazilian Portuguese, which we nickname BERTimbau. We evaluate our models on three downstream NLP tasks: sentence textual similarity, recognizing textual entailment, and named entity recognition. Our models improve the state-of-the-art in all of these tasks, outperforming Multilingual BERT and confirming the effectiveness of large pretrained LMs for Portuguese. We release our models to the community hoping to provide strong baselines for future NLP research: .},
    booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
    pages = {403–417},
    numpages = {15},
    location = {Rio Grande, Brazil}
}

@article{Pearson1895,
    author = {Pearson, Karl  and Galton, Francis },
    title = {VII. Note on regression and inheritance in the case of two parents},
    journal = {Proceedings of the Royal Society of London},
    volume = {58},
    number = {347-352},
    pages = {240-242},
    year = {1895},
    doi = {10.1098/rspl.1895.0041},
    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspl.1895.0041},
    eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1895.0041}
    ,
    abstract = { Consider a population in which sexual selection and natural selection may or may not be taking place. Assume only that the deviations from the mean in the case of any organ of any generation follow exactly or closely the normal law of frequency, then the following expressions may be shown to give the law of inheritance of the population. }
}

@article{jastrzkebowska2021opening,
  title={Opening tasks, opening minds--a rediscovery of the open-ended approach},
  author={Jastrz{\k{e}}bowska, Kalina},
  journal={Problemy Wczesnej Edukacji},
  volume={17},
  number={1 (52)},
  pages={181--195},
  year={2021},
  publisher={Uniwersytet Gda{\'n}ski}
}

@article{burrows2015eras,
  title={The eras and trends of automatic short answer grading},
  author={Burrows, Steven and Gurevych, Iryna and Stein, Benno},
  journal={International journal of artificial intelligence in education},
  volume={25},
  pages={60--117},
  year={2015},
  publisher={Springer}
}

@article{farea2025evaluation,
author = {Farea, Amer and Yang, Zhen and Duong, Kien and Perera, Nadeesha and Emmert-Streib, Frank},
title = {Evaluation of Question Answering Systems: Complexity of Judging a Natural Language},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3744663},
doi = {10.1145/3744663},
abstract = {Question answering (QA) systems are a leading and rapidly advancing field of natural language processing (NLP) research. One of their key advantages is that they enable more natural interactions between humans and machines, such as in virtual assistants or search engines. Over the past few decades, many QA systems have been developed to handle diverse QA tasks. However, the evaluation of these systems is intricate, as many of the available evaluation scores are not task-agnostic. Furthermore, translating human judgment into measurable metrics continues to be an open issue. These complexities add challenges to their assessment. This survey provides a systematic overview of evaluation scores and introduces a taxonomy with two main branches: Human-Centric Evaluation Scores (HCES) and Automatic Evaluation Scores (AES). Since many of these scores were originally designed for specific tasks but have been applied more generally, we also cover the basics of QA frameworks and core paradigms to provide a deeper understanding of their capabilities and limitations. Lastly, we discuss benchmark datasets that are critical for conducting systematic evaluations across various QA tasks.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jun,
keywords = {natural language processing, question answering, evaluation scores, artificial intelligence, neural networks, deep learning}
}

@inproceedings{han-etal-2024-rag,
    title = "{RAG}-{QA} Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
    author = "Han, Rujun  and
      Zhang, Yuhao  and
      Qi, Peng  and
      Xu, Yumo  and
      Wang, Jenyuan  and
      Liu, Lan  and
      Wang, William Yang  and
      Min, Bonan  and
      Castelli, Vittorio",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.249/",
    doi = "10.18653/v1/2024.emnlp-main.249",
    pages = "4354--4374",
    abstract = "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA{'}s answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3{\%} of the most competitive LLM{'}s answers are preferred to LFRQA{'}s answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research."
}

@inproceedings{krishna-etal-2021-hurdles,
    title = "Hurdles to Progress in Long-form Question Answering",
    author = "Krishna, Kalpesh  and
      Roy, Aurko  and
      Iyyer, Mohit",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.393/",
    doi = "10.18653/v1/2021.naacl-main.393",
    pages = "4940--4957",
    abstract = "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system{'}s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81{\%} of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future."
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{gao2023precise,
    title = "Precise Zero-Shot Dense Retrieval without Relevance Labels",
    author = "Gao, Luyu  and
      Ma, Xueguang  and
      Lin, Jimmy  and
      Callan, Jamie",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.99/",
    doi = "10.18653/v1/2023.acl-long.99",
    pages = "1762--1777",
    abstract = "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is ``fake'' and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder{'}s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn)."
}
